{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8260db4-4f2e-4919-b5a3-ef5f52622c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde932b6-ee3b-40af-9efa-a565b6ae2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdf = pl.read_csv(\"datasets/PDF_All_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6faaad89-a615-4b39-8123-83362b239016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file_path is dropped before duplication is removed because malicious samples are\n",
    "# Collected from different sources and might be the same but saved/sent with a different name, therefore only the name will be \n",
    "# different which is not all that important.\n",
    "df_pdf = df_pdf.drop(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d191af-4b02-45eb-8f53-3b9252b7d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_pdf Size = 19296\n",
      "After removing duplicates = 15353\n",
      "Number of Rows dropped = 3943\n"
     ]
    }
   ],
   "source": [
    "original_height = df_pdf.height\n",
    "df_pdf = df_pdf.unique()\n",
    "ch1_height = df_pdf.height\n",
    "print(f\"df_pdf Size = {original_height}\\nAfter removing duplicates = {ch1_height}\\nNumber of Rows dropped = {original_height-ch1_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b572c31-e8eb-49ae-9a12-1d5d7d901f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df_pdf.select(pl.col(pl.Int64, pl.Float64)).columns\n",
    "X = df_pdf.select(numeric_cols).drop(\"label\")\n",
    "y = df_pdf[\"label\"]\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c90509a-b58a-44ee-9f71-65eebe1d84b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shapes\n",
      "Train: (7676, 40), Test: (7677, 40)\n",
      "Detected 8 empty columns: ['embedded_file_count', 'average_embedded_file_size', 'xref_count', 'xref_entries', 'submitform_count', 'jbig2decode_count', 'trailer_count', 'startxref_count']\n",
      "\n",
      "New Shapes\n",
      " Train: (7676, 32), Test: (7677, 32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Shapes\\nTrain: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "empty_cols = [\n",
    "    col for col in X_train.columns \n",
    "    if (X_train[col] == 0).all()\n",
    "]\n",
    "\n",
    "print(f\"Detected {len(empty_cols)} empty columns: {empty_cols}\")\n",
    "\n",
    "X_train = X_train.drop(empty_cols)\n",
    "X_test = X_test.drop(empty_cols)\n",
    "\n",
    "print(f\"\\nNew Shapes\\n Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a062602-6195-4312-b91a-530369382d2f",
   "metadata": {},
   "source": [
    "> [!info] Check `ocr.ipynb` for why `used_ocr=1` rows were not dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb354297-e58c-4cf4-b7ae-a1102ce85bc5",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c159b6-00f9-4792-bcb4-d2a1a00887b9",
   "metadata": {},
   "source": [
    "Since the dataset is quite small, removing outliers is not an option. Therefore the decision is taken to Winsorize the outliers. Also, based on EDA of the dataset, it is clear that the dataset is quite skewed especially when studyimg the distribution of a feature based on the label. This will negatively impact the training of the model, and therefore must be fixed before proceeding. Thus log1p Transform is used to transform the data. The function below does both processes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce39869-dd32-4c99-9d63-7743f58d90b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 32 numeric features...\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = X_train.select(pl.col(pl.Int64, pl.Float64)).columns\n",
    "\n",
    "print(f\"Processing {len(numeric_cols)} numeric features...\")\n",
    "\n",
    "caps_df = X_train.select([\n",
    "    pl.col(c).quantile(0.99).alias(c) for c in numeric_cols\n",
    "])\n",
    "caps = caps_df.to_dict(as_series=False)\n",
    "\n",
    "\n",
    "def apply_polars_preprocessing(df, caps_dict):\n",
    "    ops = []\n",
    "\n",
    "    valid_cols = [c for c in numeric_cols if c in df.columns]\n",
    "    \n",
    "    for col in valid_cols:\n",
    "        cap_val = caps_dict[col][0]\n",
    "\n",
    "        ops.append(\n",
    "            pl.col(col)\n",
    "            .clip(upper_bound=cap_val) \n",
    "            .log1p()                   \n",
    "            .alias(col)\n",
    "        )\n",
    "    return df.with_columns(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4b23ac-6d10-4fc8-8b96-5889b81f724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transformations...\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying transformations...\")\n",
    "X_train_clean = apply_polars_preprocessing(X_train, caps)\n",
    "X_test_clean = apply_polars_preprocessing(X_test, caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7859671-436d-47fc-9eb1-f5c71b7bd410",
   "metadata": {},
   "source": [
    "After Preprocessing, it is important to drop sparse columns which will not be of any use. This is preferred over Feature Reduction or Dimensioality Reduction methods because of the data distribution. The problems are not solved even with Transformtion, and must be dropped manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8003df04-ec04-4a4e-b9f3-1d372db6d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifing useless (sparse) features...\n",
      "Dropping 4 useless columns: ['encrypted', 'uses_nonstandard_port', 'launch_count', 'richmedia_count']\n",
      "\n",
      "Pipeline Complete.\n",
      "Train Shape: (7676, 28)\n",
      "Test Shape:  (7677, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Identifing useless (sparse) features...\")\n",
    "\n",
    "total_rows = X_train_clean.height\n",
    "useless_cols = []\n",
    "\n",
    "zero_counts = X_train_clean.select([\n",
    "    (pl.col(c) == 0).sum().alias(c) for c in numeric_cols\n",
    "]).to_dict(as_series=False)\n",
    "\n",
    "for col, count in zero_counts.items():\n",
    "    if (count[0] / total_rows) > 0.99: \n",
    "        useless_cols.append(col)\n",
    "\n",
    "print(f\"Dropping {len(useless_cols)} useless columns: {useless_cols}\")\n",
    "\n",
    "# Drop from both\n",
    "X_train_final = X_train_clean.drop(useless_cols)\n",
    "X_test_final = X_test_clean.drop(useless_cols)\n",
    "\n",
    "# --- FINAL VALIDATION ---\n",
    "print(\"\\nPipeline Complete.\")\n",
    "print(f\"Train Shape: {X_train_final.shape}\")\n",
    "print(f\"Test Shape:  {X_test_final.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
